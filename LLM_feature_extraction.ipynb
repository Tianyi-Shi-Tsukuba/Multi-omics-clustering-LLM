{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7y21_ZeVqOd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
        "model = AutoModel.from_pretrained(\"dmis-lab/biobert-v1.1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "input_file = \"HNSC_patho.csv\"\n",
        "output_file = \"/HNSC_embeddings.csv\"\n",
        "df = pd.read_csv(input_file)"
      ],
      "metadata": {
        "id": "OSUlTicXWAIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "text_column = \"text\"\n",
        "if text_column not in df.columns:\n",
        "    raise ValueError(f\"'{text_column}' is not in text.\")\n",
        "\n",
        "def get_embedding_for_text(text):\n",
        "    max_length = 512\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        sentence_embedding = outputs.pooler_output.squeeze().numpy()\n",
        "    return sentence_embedding\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "for i, text in enumerate(df[text_column]):\n",
        "    if pd.isna(text):\n",
        "        embeddings.append([None] * 768)\n",
        "        continue\n",
        "\n",
        "    if len(tokenizer.tokenize(text)) > 512:\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        chunks = [tokens[i:i + 512] for i in range(0, len(tokens), 512)]  # 分块\n",
        "        chunk_embeddings = []\n",
        "        for chunk in chunks:\n",
        "            chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
        "            chunk_embedding = get_embedding_for_text(chunk_text)\n",
        "            chunk_embeddings.append(chunk_embedding)\n",
        "        sentence_embedding = np.mean(chunk_embeddings, axis=0)\n",
        "    else:\n",
        "        sentence_embedding = get_embedding_for_text(text)\n",
        "\n",
        "    embeddings.append(sentence_embedding)\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"Processing {i + 1}/{len(df)} text\")\n",
        "\n",
        "embedding_columns = [f\"embedding_{i}\" for i in range(768)]\n",
        "embedding_df = pd.DataFrame(embeddings, columns=embedding_columns)"
      ],
      "metadata": {
        "id": "371ZAUywWFfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_columns = [f\"embedding_{i}\" for i in range(768)]\n",
        "embedding_df = pd.DataFrame(embeddings, columns=embedding_columns)\n",
        "result_df = pd.concat([df, embedding_df], axis=1)\n",
        "result_df.to_csv(output_file, index=False)\n",
        "print(f\"Saved {output_file}\")"
      ],
      "metadata": {
        "id": "DxMMjeaxWbFh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}